{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_torch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMDh76QI2ukT"
      },
      "source": [
        "Fuente: https://github.com/esansano/decharlas_deep_learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAIa_XCB3GXg"
      },
      "source": [
        "https://medium.com/metadatos/todo-lo-que-necesitas-saber-sobre-el-descenso-del-gradiente-aplicado-a-redes-neuronales-19bdbb706a78\n",
        "\n",
        "Jugando con rrnn\n",
        "http://playground.tensorflow.org/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GZIAgbUOuo4v",
        "outputId": "b902ad66-4d0c-444e-a35a-107cf4e3cab3"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv('https://raw.githubusercontent.com/esansano/decharlas_deep_learning/master/train.csv', header=0).values\n",
        "x = train[:, 1:] / 255.0\n",
        "y = train[:, 0]\n",
        "enc = OneHotEncoder()\n",
        "y = enc.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=0)\n",
        "\n",
        "# Neural network dimensions\n",
        "in_dim = x_train.shape[1]\n",
        "hid_dim_1 = 1000\n",
        "hid_dim_2 = 500\n",
        "hid_dim_3 = 100\n",
        "out_dim = 10\n",
        "\n",
        "# Batch size\n",
        "batch = 1000\n",
        "\n",
        "# Training data\n",
        "x = Variable(torch.from_numpy(x_train), requires_grad=False).type(torch.FloatTensor).cuda()\n",
        "y = Variable(torch.from_numpy(y_train), requires_grad=False).type(torch.FloatTensor).cuda()\n",
        "\n",
        "# Test data\n",
        "xt = Variable(torch.from_numpy(x_test), requires_grad=False).type(torch.FloatTensor).cuda()\n",
        "\n",
        "# Model definition\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_dim, hid_dim_1),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(hid_dim_1, hid_dim_2),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(hid_dim_2, hid_dim_3),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(hid_dim_3, out_dim)\n",
        ").cuda()\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.MSELoss().cuda()\n",
        "\n",
        "# Optimization function\n",
        "learning_rate = 1\n",
        "optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
        "loss_list = []\n",
        "acc_list = []\n",
        "max_epochs = 100\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for i in range(0, x.size()[0], batch):\n",
        "        # Forward pass\n",
        "        y_pred = model(x[i:(i+batch), :])\n",
        "        loss = criterion(y_pred, y[i:(i+batch), :])\n",
        "#        epoch_loss += loss.data[0]\n",
        "        epoch_loss += loss.data.item()\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "    # Test accuracy\n",
        "    yt_pred = model(xt).data.cpu().numpy()\n",
        "    accuracy = np.sum(np.argmax(y_test, axis=1) == np.argmax(yt_pred, axis=1)) / yt_pred.shape[0]\n",
        "\n",
        "    print('Epoch: %d, Loss: %f, Accuracy: %f' % (epoch + 1, epoch_loss, accuracy))\n",
        "    loss_list.append(epoch_loss)\n",
        "    acc_list.append(accuracy)\n",
        "\n",
        "# Plot loss and accuracy\n",
        "plt.scatter(range(max_epochs), acc_list, c=['green'], s=4)\n",
        "plt.scatter(range(max_epochs), loss_list, c=['red'], s=4)\n",
        "plt.title('GPU training')\n",
        "plt.axis([0, max_epochs, 0, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Loss: 2.447756, Accuracy: 0.650000\n",
            "Epoch: 2, Loss: 1.865683, Accuracy: 0.783622\n",
            "Epoch: 3, Loss: 1.520135, Accuracy: 0.819697\n",
            "Epoch: 4, Loss: 1.240925, Accuracy: 0.848052\n",
            "Epoch: 5, Loss: 1.063838, Accuracy: 0.861472\n",
            "Epoch: 6, Loss: 0.914874, Accuracy: 0.880231\n",
            "Epoch: 7, Loss: 0.787355, Accuracy: 0.896898\n",
            "Epoch: 8, Loss: 0.700557, Accuracy: 0.898341\n",
            "Epoch: 9, Loss: 0.647361, Accuracy: 0.908009\n",
            "Epoch: 10, Loss: 0.590711, Accuracy: 0.912626\n",
            "Epoch: 11, Loss: 0.552806, Accuracy: 0.917893\n",
            "Epoch: 12, Loss: 0.520310, Accuracy: 0.921429\n",
            "Epoch: 13, Loss: 0.492460, Accuracy: 0.924820\n",
            "Epoch: 14, Loss: 0.468553, Accuracy: 0.928788\n",
            "Epoch: 15, Loss: 0.447193, Accuracy: 0.931457\n",
            "Epoch: 16, Loss: 0.428054, Accuracy: 0.934055\n",
            "Epoch: 17, Loss: 0.410681, Accuracy: 0.935786\n",
            "Epoch: 18, Loss: 0.394872, Accuracy: 0.938167\n",
            "Epoch: 19, Loss: 0.380557, Accuracy: 0.940548\n",
            "Epoch: 20, Loss: 0.367064, Accuracy: 0.942352\n",
            "Epoch: 21, Loss: 0.355019, Accuracy: 0.943939\n",
            "Epoch: 22, Loss: 0.343176, Accuracy: 0.945815\n",
            "Epoch: 23, Loss: 0.332414, Accuracy: 0.947258\n",
            "Epoch: 24, Loss: 0.322133, Accuracy: 0.947908\n",
            "Epoch: 25, Loss: 0.312457, Accuracy: 0.949062\n",
            "Epoch: 26, Loss: 0.303553, Accuracy: 0.950000\n",
            "Epoch: 27, Loss: 0.294869, Accuracy: 0.951659\n",
            "Epoch: 28, Loss: 0.286818, Accuracy: 0.952309\n",
            "Epoch: 29, Loss: 0.279362, Accuracy: 0.953102\n",
            "Epoch: 30, Loss: 0.271907, Accuracy: 0.954257\n",
            "Epoch: 31, Loss: 0.265381, Accuracy: 0.955195\n",
            "Epoch: 32, Loss: 0.258123, Accuracy: 0.955195\n",
            "Epoch: 33, Loss: 0.255496, Accuracy: 0.957071\n",
            "Epoch: 34, Loss: 0.246989, Accuracy: 0.955628\n",
            "Epoch: 35, Loss: 0.245804, Accuracy: 0.958442\n",
            "Epoch: 36, Loss: 0.234776, Accuracy: 0.957720\n",
            "Epoch: 37, Loss: 0.231451, Accuracy: 0.959957\n",
            "Epoch: 38, Loss: 0.224832, Accuracy: 0.958874\n",
            "Epoch: 39, Loss: 0.222683, Accuracy: 0.960967\n",
            "Epoch: 40, Loss: 0.216005, Accuracy: 0.959957\n",
            "Epoch: 41, Loss: 0.213065, Accuracy: 0.962049\n",
            "Epoch: 42, Loss: 0.207151, Accuracy: 0.961977\n",
            "Epoch: 43, Loss: 0.204129, Accuracy: 0.963636\n",
            "Epoch: 44, Loss: 0.199602, Accuracy: 0.962554\n",
            "Epoch: 45, Loss: 0.196510, Accuracy: 0.964719\n",
            "Epoch: 46, Loss: 0.192122, Accuracy: 0.963781\n",
            "Epoch: 47, Loss: 0.188472, Accuracy: 0.965007\n",
            "Epoch: 48, Loss: 0.185369, Accuracy: 0.964502\n",
            "Epoch: 49, Loss: 0.181929, Accuracy: 0.965729\n",
            "Epoch: 50, Loss: 0.179152, Accuracy: 0.965224\n",
            "Epoch: 51, Loss: 0.175018, Accuracy: 0.965873\n",
            "Epoch: 52, Loss: 0.172730, Accuracy: 0.965729\n",
            "Epoch: 53, Loss: 0.169186, Accuracy: 0.966378\n",
            "Epoch: 54, Loss: 0.167172, Accuracy: 0.966306\n",
            "Epoch: 55, Loss: 0.163355, Accuracy: 0.966811\n",
            "Epoch: 56, Loss: 0.161330, Accuracy: 0.966883\n",
            "Epoch: 57, Loss: 0.157941, Accuracy: 0.967244\n",
            "Epoch: 58, Loss: 0.156558, Accuracy: 0.967172\n",
            "Epoch: 59, Loss: 0.152987, Accuracy: 0.967532\n",
            "Epoch: 60, Loss: 0.151681, Accuracy: 0.967965\n",
            "Epoch: 61, Loss: 0.148351, Accuracy: 0.968326\n",
            "Epoch: 62, Loss: 0.147244, Accuracy: 0.968326\n",
            "Epoch: 63, Loss: 0.143960, Accuracy: 0.968903\n",
            "Epoch: 64, Loss: 0.142588, Accuracy: 0.969120\n",
            "Epoch: 65, Loss: 0.139637, Accuracy: 0.969120\n",
            "Epoch: 66, Loss: 0.138214, Accuracy: 0.969481\n",
            "Epoch: 67, Loss: 0.135380, Accuracy: 0.969625\n",
            "Epoch: 68, Loss: 0.134170, Accuracy: 0.969408\n",
            "Epoch: 69, Loss: 0.131457, Accuracy: 0.969769\n",
            "Epoch: 70, Loss: 0.130654, Accuracy: 0.969769\n",
            "Epoch: 71, Loss: 0.127977, Accuracy: 0.970346\n",
            "Epoch: 72, Loss: 0.127291, Accuracy: 0.970563\n",
            "Epoch: 73, Loss: 0.124646, Accuracy: 0.970491\n",
            "Epoch: 74, Loss: 0.123772, Accuracy: 0.970851\n",
            "Epoch: 75, Loss: 0.121108, Accuracy: 0.970779\n",
            "Epoch: 76, Loss: 0.120004, Accuracy: 0.971284\n",
            "Epoch: 77, Loss: 0.117890, Accuracy: 0.971140\n",
            "Epoch: 78, Loss: 0.116990, Accuracy: 0.971429\n",
            "Epoch: 79, Loss: 0.115066, Accuracy: 0.971429\n",
            "Epoch: 80, Loss: 0.114059, Accuracy: 0.971573\n",
            "Epoch: 81, Loss: 0.111881, Accuracy: 0.972006\n",
            "Epoch: 82, Loss: 0.110720, Accuracy: 0.972222\n",
            "Epoch: 83, Loss: 0.108996, Accuracy: 0.972294\n",
            "Epoch: 84, Loss: 0.108123, Accuracy: 0.972511\n",
            "Epoch: 85, Loss: 0.106321, Accuracy: 0.972439\n",
            "Epoch: 86, Loss: 0.105440, Accuracy: 0.972655\n",
            "Epoch: 87, Loss: 0.103634, Accuracy: 0.972439\n",
            "Epoch: 88, Loss: 0.102902, Accuracy: 0.972944\n",
            "Epoch: 89, Loss: 0.101031, Accuracy: 0.972655\n",
            "Epoch: 90, Loss: 0.100293, Accuracy: 0.973304\n",
            "Epoch: 91, Loss: 0.098490, Accuracy: 0.973016\n",
            "Epoch: 92, Loss: 0.097969, Accuracy: 0.973449\n",
            "Epoch: 93, Loss: 0.096381, Accuracy: 0.973160\n",
            "Epoch: 94, Loss: 0.095787, Accuracy: 0.974026\n",
            "Epoch: 95, Loss: 0.094012, Accuracy: 0.973521\n",
            "Epoch: 96, Loss: 0.093389, Accuracy: 0.974315\n",
            "Epoch: 97, Loss: 0.091778, Accuracy: 0.973665\n",
            "Epoch: 98, Loss: 0.091291, Accuracy: 0.974603\n",
            "Epoch: 99, Loss: 0.089719, Accuracy: 0.973810\n",
            "Epoch: 100, Loss: 0.088960, Accuracy: 0.974820\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-au8DUJk2tMM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7urQXdEYxI4x"
      },
      "source": [
        "numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dhp-A2RxIeC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aeed756-df99-4fac-c2d8-d6c8eb21c9be"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "# Tensor dimensions\n",
        "d1, d2 = 4, 5\n",
        "\n",
        "# Initialize three tensors\n",
        "t1 = np.random.rand(d1, d2)\n",
        "t2 = np.random.rand(d1, d2)\n",
        "t3 = np.random.rand(d1, d2)\n",
        "\n",
        "# Computational graph\n",
        "a = t1 + t2\n",
        "b = a * t3\n",
        "c = np.sum(b) # 8.80812307798\n",
        "\n",
        "grad_c = 1.0\n",
        "grad_b = grad_c * np.ones((d1, d2))\n",
        "grad_a = grad_b * t3\n",
        "grad_t3 = grad_b * a\n",
        "grad_t1 = grad_a.copy()\n",
        "grad_t2 = grad_a.copy()\n",
        "grad_t1,grad_t2,grad_t3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.3595079 , 0.43703195, 0.6976312 , 0.06022547, 0.66676672],\n",
              "        [0.67063787, 0.21038256, 0.1289263 , 0.31542835, 0.36371077],\n",
              "        [0.57019677, 0.43860151, 0.98837384, 0.10204481, 0.20887676],\n",
              "        [0.16130952, 0.65310833, 0.2532916 , 0.46631077, 0.24442559]]),\n",
              " array([[0.3595079 , 0.43703195, 0.6976312 , 0.06022547, 0.66676672],\n",
              "        [0.67063787, 0.21038256, 0.1289263 , 0.31542835, 0.36371077],\n",
              "        [0.57019677, 0.43860151, 0.98837384, 0.10204481, 0.20887676],\n",
              "        [0.16130952, 0.65310833, 0.2532916 , 0.46631077, 0.24442559]]),\n",
              " array([[1.52743185, 1.51434793, 1.06424274, 1.32541236, 0.54192923],\n",
              "        [1.28581513, 0.5809405 , 1.83644192, 1.48551108, 0.79810346],\n",
              "        [1.05628065, 1.30312861, 1.02419489, 1.49403059, 0.08982586],\n",
              "        [0.7047648 , 0.63231412, 1.44955384, 1.72190483, 1.55183245]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S80PpwwgxgM5"
      },
      "source": [
        "torch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCa0PzADxin1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6aa92f5-a847-465d-cf58-b041930b743c"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Tensor dimensions\n",
        "d1, d2 = 4, 5\n",
        "\n",
        "# Computational graph definition with Variables\n",
        "t1 = Variable(torch.randn(d1, d2), requires_grad=True)\n",
        "t2 = Variable(torch.randn(d1, d2), requires_grad=True)\n",
        "t3 = Variable(torch.randn(d1, d2), requires_grad=True)\n",
        "a = t1 + t2\n",
        "b = a * t3\n",
        "c = torch.sum(b)\n",
        "\n",
        "# Gradient computation\n",
        "c.backward()\n",
        "\n",
        "grad_t1 = t1.grad.data\n",
        "grad_t2 = t2.grad.data\n",
        "grad_t3 = t3.grad.data\n",
        "grad_t1,grad_t2,grad_t3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 1.5091,  2.0820,  1.7067,  2.3804,  1.9415],\n",
              "         [ 0.7915, -0.0203, -0.4372,  1.6459, -1.3602],\n",
              "         [ 0.3446,  0.5199, -0.3656, -1.3024,  0.0994],\n",
              "         [ 0.4418,  0.2469,  0.0769,  0.3380,  0.4544]]),\n",
              " tensor([[ 1.5091,  2.0820,  1.7067,  2.3804,  1.9415],\n",
              "         [ 0.7915, -0.0203, -0.4372,  1.6459, -1.3602],\n",
              "         [ 0.3446,  0.5199, -0.3656, -1.3024,  0.0994],\n",
              "         [ 0.4418,  0.2469,  0.0769,  0.3380,  0.4544]]),\n",
              " tensor([[-0.6861, -1.0399,  0.3902,  0.0073,  0.8044],\n",
              "         [-2.0054, -0.9144,  1.2976,  1.0624,  1.3842],\n",
              "         [ 1.9510, -0.3250,  0.0640,  1.7749,  0.8772],\n",
              "         [-1.3386, -0.5641,  1.0652,  2.4505,  1.3790]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGXfbzf7x3Ct"
      },
      "source": [
        "tensorflow\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDM3k1dyx5Me",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97124829-5867-4c61-ccd2-258cf5160388"
      },
      "source": [
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "np.random.seed(0)\n",
        "\n",
        "# Tensor dimensions\n",
        "d1, d2 = 4, 5\n",
        "\n",
        "# Computational graph definition\n",
        "t1 = tf.placeholder(tf.float32)\n",
        "t2 = tf.placeholder(tf.float32)\n",
        "t3 = tf.placeholder(tf.float32)\n",
        "a = t1 + t2\n",
        "b = a * t3\n",
        "c = tf.reduce_sum(b)\n",
        "\n",
        "# Gradients computation definition\n",
        "grad_t1, grad_t2, grad_t3 = tf.gradients(c, [t1, t2, t3])\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    values = {\n",
        "        t1: np.random.rand(d1, d2),\n",
        "        t2: np.random.rand(d1, d2),\n",
        "        t3: np.random.rand(d1, d2),\n",
        "    }\n",
        "    out = sess.run([c, grad_t1, grad_t2, grad_t3], feed_dict=values)\n",
        "\n",
        "    c_val, grad_t1_val, grad_t2_val, grad_t3_val = out"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0ko979j0aiN"
      },
      "source": [
        "keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azVSlwGj0b7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aece90ba-45ea-4e03-d093-aae18a0ca431"
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.2992 - acc: 0.9119\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.1461 - acc: 0.9569\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.1076 - acc: 0.9676\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.0886 - acc: 0.9728\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 53us/sample - loss: 0.0744 - acc: 0.9762\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:2426: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07638371645715088, 0.9761]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQkH3Kju1G43"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNSPEd3t1IMc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd380ea-41cd-48bb-c720-785b018d149d"
      },
      "source": [
        "  \n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# Neural network dimensions\n",
        "n, in_dim, hid_dim, out_dim = 50, 100, 500, 10\n",
        "\n",
        "# Training data\n",
        "x = Variable(torch.randn(n, in_dim), requires_grad=False)\n",
        "y = Variable(torch.randn(n, out_dim), requires_grad=False)\n",
        "\n",
        "# Model definition\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_dim, hid_dim),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(hid_dim, out_dim)\n",
        ")\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "# Optimization function\n",
        "learning_rate = 1e-6\n",
        "optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
        "\n",
        "for epoch in range(500):\n",
        "\n",
        "    # Forward pass\n",
        "    y_pred = model(x)\n",
        "    loss = criterion(y_pred, y)\n",
        "\n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "    print(epoch,loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor(0.9514, grad_fn=<MseLossBackward>)\n",
            "1 tensor(0.9514, grad_fn=<MseLossBackward>)\n",
            "2 tensor(0.9514, grad_fn=<MseLossBackward>)\n",
            "3 tensor(0.9514, grad_fn=<MseLossBackward>)\n",
            "4 tensor(0.9514, grad_fn=<MseLossBackward>)\n",
            "5 tensor(0.9514, grad_fn=<MseLossBackward>)\n",
            "6 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "7 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "8 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "9 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "10 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "11 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "12 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "13 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "14 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "15 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "16 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "17 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "18 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "19 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "20 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "21 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "22 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "23 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "24 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "25 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "26 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "27 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "28 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "29 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "30 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "31 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "32 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "33 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "34 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "35 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "36 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "37 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "38 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "39 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "40 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "41 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "42 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "43 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "44 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "45 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "46 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "47 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "48 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "49 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "50 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "51 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "52 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "53 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "54 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "55 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "56 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "57 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "58 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "59 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "60 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "61 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "62 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "63 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "64 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "65 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "66 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "67 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "68 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "69 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "70 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "71 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "72 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "73 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "74 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "75 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "76 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "77 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "78 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "79 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "80 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "81 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "82 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "83 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "84 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "85 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "86 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "87 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "88 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "89 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "90 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "91 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "92 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "93 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "94 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "95 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "96 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "97 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "98 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "99 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "100 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "101 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "102 tensor(0.9513, grad_fn=<MseLossBackward>)\n",
            "103 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "104 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "105 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "106 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "107 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "108 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "109 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "110 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "111 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "112 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "113 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "114 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "115 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "116 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "117 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "118 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "119 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "120 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "121 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "122 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "123 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "124 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "125 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "126 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "127 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "128 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "129 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "130 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "131 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "132 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "133 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "134 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "135 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "136 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "137 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "138 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "139 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "140 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "141 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "142 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "143 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "144 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "145 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "146 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "147 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "148 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "149 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "150 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "151 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "152 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "153 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "154 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "155 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "156 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "157 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "158 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "159 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "160 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "161 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "162 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "163 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "164 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "165 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "166 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "167 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "168 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "169 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "170 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "171 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "172 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "173 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "174 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "175 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "176 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "177 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "178 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "179 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "180 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "181 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "182 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "183 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "184 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "185 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "186 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "187 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "188 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "189 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "190 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "191 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "192 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "193 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "194 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "195 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "196 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "197 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "198 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "199 tensor(0.9512, grad_fn=<MseLossBackward>)\n",
            "200 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "201 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "202 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "203 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "204 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "205 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "206 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "207 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "208 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "209 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "210 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "211 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "212 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "213 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "214 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "215 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "216 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "217 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "218 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "219 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "220 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "221 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "222 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "223 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "224 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "225 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "226 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "227 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "228 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "229 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "230 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "231 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "232 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "233 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "234 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "235 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "236 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "237 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "238 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "239 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "240 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "241 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "242 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "243 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "244 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "245 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "246 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "247 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "248 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "249 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "250 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "251 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "252 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "253 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "254 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "255 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "256 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "257 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "258 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "259 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "260 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "261 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "262 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "263 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "264 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "265 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "266 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "267 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "268 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "269 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "270 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "271 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "272 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "273 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "274 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "275 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "276 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "277 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "278 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "279 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "280 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "281 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "282 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "283 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "284 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "285 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "286 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "287 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "288 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "289 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "290 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "291 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "292 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "293 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "294 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "295 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "296 tensor(0.9511, grad_fn=<MseLossBackward>)\n",
            "297 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "298 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "299 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "300 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "301 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "302 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "303 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "304 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "305 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "306 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "307 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "308 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "309 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "310 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "311 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "312 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "313 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "314 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "315 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "316 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "317 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "318 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "319 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "320 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "321 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "322 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "323 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "324 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "325 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "326 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "327 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "328 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "329 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "330 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "331 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "332 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "333 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "334 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "335 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "336 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "337 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "338 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "339 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "340 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "341 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "342 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "343 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "344 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "345 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "346 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "347 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "348 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "349 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "350 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "351 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "352 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "353 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "354 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "355 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "356 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "357 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "358 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "359 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "360 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "361 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "362 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "363 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "364 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "365 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "366 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "367 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "368 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "369 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "370 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "371 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "372 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "373 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "374 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "375 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "376 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "377 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "378 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "379 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "380 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "381 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "382 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "383 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "384 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "385 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "386 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "387 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "388 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "389 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "390 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "391 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "392 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "393 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "394 tensor(0.9510, grad_fn=<MseLossBackward>)\n",
            "395 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "396 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "397 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "398 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "399 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "400 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "401 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "402 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "403 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "404 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "405 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "406 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "407 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "408 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "409 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "410 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "411 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "412 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "413 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "414 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "415 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "416 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "417 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "418 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "419 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "420 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "421 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "422 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "423 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "424 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "425 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "426 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "427 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "428 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "429 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "430 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "431 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "432 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "433 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "434 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "435 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "436 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "437 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "438 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "439 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "440 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "441 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "442 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "443 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "444 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "445 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "446 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "447 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "448 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "449 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "450 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "451 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "452 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "453 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "454 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "455 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "456 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "457 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "458 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "459 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "460 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "461 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "462 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "463 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "464 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "465 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "466 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "467 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "468 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "469 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "470 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "471 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "472 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "473 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "474 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "475 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "476 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "477 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "478 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "479 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "480 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "481 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "482 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "483 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "484 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "485 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "486 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "487 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "488 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "489 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "490 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "491 tensor(0.9509, grad_fn=<MseLossBackward>)\n",
            "492 tensor(0.9508, grad_fn=<MseLossBackward>)\n",
            "493 tensor(0.9508, grad_fn=<MseLossBackward>)\n",
            "494 tensor(0.9508, grad_fn=<MseLossBackward>)\n",
            "495 tensor(0.9508, grad_fn=<MseLossBackward>)\n",
            "496 tensor(0.9508, grad_fn=<MseLossBackward>)\n",
            "497 tensor(0.9508, grad_fn=<MseLossBackward>)\n",
            "498 tensor(0.9508, grad_fn=<MseLossBackward>)\n",
            "499 tensor(0.9508, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}